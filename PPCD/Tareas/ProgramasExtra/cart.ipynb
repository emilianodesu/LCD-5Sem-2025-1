{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mendoza Hernandez Carlos Emiliano**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea basica de este algoritmo es dividir repetidamente los registros disponibles buscando maximizar la *homogeneidad* en los conjuntos obtenidos en dicha division.\n",
    "\n",
    "Es un modelo jerarquico de aprendizaje que puede ser usado tanto para clasificacion como para regresion: permiten explorar relaciones complejas entre entradas y salidas sin necesidad de hacer suposiciones sobre los datos. Los arboles de decision pueden verse como una funcion $f$ que realiza una estimacion de un *mapeo* desde un espacio de entrada $X$ hacia un espacio objetivo $y: y=f(X)$. Para el caso de la regresion, el espacio objetivo $y$ es numerico: $y \\in \\mathbb{R}$; para la clasificacion, los valores son categoricos: $y \\in \\{ C_1, C_2,... \\}$, se trata de un modelo de aprendizaje supervisado; por tanto, se tienen muestras con la salida esperada. La representacion mas comun para CART es un arbol binario.\n",
    "\n",
    "Un arbol de decision divide el espacio $X$ en regiones locales utilizando alguna medida de distancia y el objetivo es determinar particiones bien separadas y homogeneas. Las regiones se dividen de acuerdo a preguntas de prueba y, con esto, se obtiene una division $n$-aria, de forma que mientras se recorre el arbol, en cada nodo se toman decisiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "**Algorithm 1.1** Obtencion de particiones\n",
    "\n",
    "--------------\n",
    "\n",
    "Elegir una variable $x_i$\n",
    "\n",
    "+ Elegir algun valor $s_i$ para $x_i$ que divida los datos de entrenamiento en dos particiones (no necesariamente iguales)\n",
    "+ Medir la *pureza* (homogeneidad) resultante en cada particion\n",
    "+ Usar otro valor $s_j$ para $x_i$ buscando incrementar la pureza de las particiones hasta alcanzar el umbral de *pureza aceptable*\n",
    "\n",
    "Repetir el proceso de particion con una variable diferente (posiblemente alguna usada previamente) cada valor obtenido se convierte en un nodo en el arbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar desde cero el algoritmo *Classification and Regrerssion Trees* (CART), incluyendo al menos las m√©tricas de pureza:\n",
    "1. *Estimate of Positive Correctness*\n",
    "2. *Variance reduction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grado de impureza de Gini\n",
    "\n",
    "El indice de impureza en un rectangulo $A$ que contiene $m$ clases, se calcula como:\n",
    "\n",
    "$$I(A) = 1-\\sum_{i=1}^{m} p_i^2$$\n",
    "\n",
    "Con $p$ la proporcion de casos en el rectangulo $A$ que pertenecen a la clase $i$.\n",
    "\n",
    "### Grado de entropia\n",
    "\n",
    "El grado de entropia en un area $A$ que contiene $m$ clases, se calcula como:\n",
    "\n",
    "$$E(A) = \\sum_{i=1}^{m} p_i \\times \\log_2 (p_i)$$\n",
    "\n",
    "Con $p$ la proporcion de casos en $A$ que pertenecen a la clase $i$.\n",
    "\n",
    "### Estimate of postive correctness\n",
    "\n",
    "$$EPC = \\frac{|L|}{|D|} \\cdot p_L + \\frac{|R|}{|D|} \\cdot p_R$$\n",
    "\n",
    "### Variance reduction\n",
    "\n",
    "$$VR = Var(D)-\\left( \\frac{|L|}{|D|} \\cdot Var(L) + \\frac{|R|}{|D|} \\cdot Var(R) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, metric='gini'):\n",
    "        self.tree = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.metric = metric\n",
    "\n",
    "    def gini_impurity(self, y, left, right):\n",
    "        def gini(idx):\n",
    "            if len(idx) == 0:\n",
    "                return 0\n",
    "            _, counts = np.unique(y[idx], return_counts=True)\n",
    "            p = counts / len(idx)\n",
    "            return 1 - np.sum(p**2)\n",
    "        left_gini = gini(left)\n",
    "        right_gini = gini(right)\n",
    "        total = len(left) + len(right)\n",
    "        weighted_gini = (len(left) / total) * left_gini + (len(right) / total) * right_gini\n",
    "        return 1 - weighted_gini\n",
    "    \n",
    "    def entropy_degree(self, y, left, right):\n",
    "        def entropy(idx):\n",
    "            if len(idx) == 0:\n",
    "                return 0\n",
    "            _, counts = np.unique(y[idx], return_counts=True)\n",
    "            p = counts / len(idx)\n",
    "            return -np.sum(p * np.log2(p))\n",
    "        left_entropy = entropy(left)\n",
    "        right_entropy = entropy(right)\n",
    "        total = len(left) + len(right)\n",
    "        weighted_entropy = (len(left) / total) * left_entropy + (len(right) / total) * right_entropy\n",
    "        return 1 - weighted_entropy\n",
    "    \n",
    "    def estimate_of_positive_correctness(self, y, left, right):\n",
    "        def epc(idx):\n",
    "            if len(idx) == 0:\n",
    "                return 0\n",
    "            _, counts = np.unique(y[idx], return_counts=True)\n",
    "            return counts.max() / len(idx)\n",
    "        left_epc = epc(left)\n",
    "        right_epc = epc(right)\n",
    "        return (len(left) * left_epc + len(right) * right_epc) / len(y)\n",
    "    \n",
    "    def variance_reduction(self, y, left, right):\n",
    "        def variance(idx):\n",
    "            if len(idx) == 0:\n",
    "                return 0\n",
    "            return np.var(y[idx])\n",
    "        left_variance = variance(left)\n",
    "        right_variance = variance(right)\n",
    "        weighted_variance = (len(left) / len(y)) * left_variance + (len(right) / len(y)) * right_variance\n",
    "        return np.var(y) - weighted_variance\n",
    "    \n",
    "    def find_best_split(self, X, y):\n",
    "        best_split = None\n",
    "        best_score = -np.inf\n",
    "        _, num_features = X.shape\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left = np.where(X[:, feature] <= threshold)[0]\n",
    "                right = np.where(X[:, feature] > threshold)[0]\n",
    "                if len(left) < self.min_samples_split or len(right) < self.min_samples_split:\n",
    "                    continue\n",
    "\n",
    "                if self.task == 'classification':\n",
    "                    if self.metric == 'gini':\n",
    "                        score = self.gini_impurity(y, left, right)\n",
    "                    elif self.metric == 'entropy':\n",
    "                        score = self.entropy_degree(y, left, right)\n",
    "                    elif self.metric == 'epc':\n",
    "                        score = self.estimate_of_positive_correctness(y, left, right)\n",
    "                elif self.task == 'regression':\n",
    "                    score = self.variance_reduction(y, left, right)\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_split = {\"feature\": feature, \"threshold\": threshold, \"indices\": (left, right)}\n",
    "        return best_split\n",
    "    \n",
    "    def create_leaf(self, y):\n",
    "        if self.task == 'classification':\n",
    "            return np.argmax(np.bincount(y))\n",
    "        elif self.task == 'regression':\n",
    "            return np.mean(y)\n",
    "        \n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        num_samples, _ = X.shape\n",
    "        if num_samples < self.min_samples_split or  depth >= self.max_depth:\n",
    "            return self.create_leaf(y)\n",
    "        \n",
    "        best_split = self.find_best_split(X, y)\n",
    "        if best_split is None:\n",
    "            return self.create_leaf(y)\n",
    "        \n",
    "        left, right = best_split[\"indices\"]\n",
    "        left_tree = self.build_tree(X[left], y[left], depth+1)\n",
    "        right_tree = self.build_tree(X[right], y[right], depth+1)\n",
    "        return {\"feature\": best_split[\"feature\"], \"threshold\": best_split[\"threshold\"], \"left\": left_tree, \"right\": right_tree}\n",
    "    \n",
    "    def traverse_tree(self, x, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        if x[tree[\"feature\"]] <= tree[\"threshold\"]:\n",
    "            return self.traverse_tree(x, tree[\"left\"])\n",
    "        else:\n",
    "            return self.traverse_tree(x, tree[\"right\"])\n",
    "        \n",
    "    def fit(self, X, y, task='classification'):\n",
    "        self.task = task\n",
    "        self.tree = self.build_tree(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.traverse_tree(x, self.tree) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, n_informative=3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con m√©trica Gini:\n",
      "Accuracy:  0.94\n",
      "Precision: 0.95\n",
      "Recall:    0.94\n",
      "F1 Score:  0.94\n"
     ]
    }
   ],
   "source": [
    "cart_model = CART(min_samples_split=10, max_depth=5, metric='gini')\n",
    "cart_model.fit(X_train, y_train, task='classification')\n",
    "y_pred = cart_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Resultados con m√©trica Gini:\")\n",
    "print(f\"Accuracy:  {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall:    {recall:.2f}\")\n",
    "print(f\"F1 Score:  {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados con m√©trica Entrop√≠a:\n",
      "Accuracy:  0.93\n",
      "Precision: 0.96\n",
      "Recall:    0.91\n",
      "F1 Score:  0.93\n"
     ]
    }
   ],
   "source": [
    "cart_model_entropy = CART(min_samples_split=10, max_depth=5, metric='entropy')\n",
    "cart_model_entropy.fit(X_train, y_train, task='classification')\n",
    "y_pred_entropy = cart_model_entropy.predict(X_test)\n",
    "\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "precision_entropy = precision_score(y_test, y_pred_entropy)\n",
    "recall_entropy = recall_score(y_test, y_pred_entropy)\n",
    "f1_entropy = f1_score(y_test, y_pred_entropy)\n",
    "\n",
    "print(\"\\nResultados con m√©trica Entrop√≠a:\")\n",
    "print(f\"Accuracy:  {accuracy_entropy:.2f}\")\n",
    "print(f\"Precision: {precision_entropy:.2f}\")\n",
    "print(f\"Recall:    {recall_entropy:.2f}\")\n",
    "print(f\"F1 Score:  {f1_entropy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados con m√©trica EPC:\n",
      "Accuracy:  0.92\n",
      "Precision: 0.93\n",
      "Recall:    0.90\n",
      "F1 Score:  0.91\n"
     ]
    }
   ],
   "source": [
    "cart_model_epc = CART(min_samples_split=10, max_depth=5, metric='epc')\n",
    "cart_model_epc.fit(X_train, y_train, task='classification')\n",
    "y_pred_epc = cart_model_epc.predict(X_test)\n",
    "\n",
    "accuracy_epc = accuracy_score(y_test, y_pred_epc)\n",
    "precision_epc = precision_score(y_test, y_pred_epc)\n",
    "recall_epc = recall_score(y_test, y_pred_epc)\n",
    "f1_epc = f1_score(y_test, y_pred_epc)\n",
    "\n",
    "print(\"\\nResultados con m√©trica EPC:\")\n",
    "print(f\"Accuracy:  {accuracy_epc:.2f}\")\n",
    "print(f\"Precision: {precision_epc:.2f}\")\n",
    "print(f\"Recall:    {recall_epc:.2f}\")\n",
    "print(f\"F1 Score:  {f1_epc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de regresi√≥n:\n",
      "Mean Squared Error (MSE): 1507.25\n",
      "Mean Absolute Error (MAE): 30.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "X, y = make_regression(n_samples=1000, n_features=5, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "cart_model = CART(min_samples_split=5, max_depth=5, metric='variance')\n",
    "cart_model.fit(X_train, y_train, task='regression')\n",
    "\n",
    "y_pred = cart_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"Resultados de regresi√≥n:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
